{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d2bf48cd9d204f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T02:43:06.447792Z",
     "start_time": "2024-12-17T02:43:06.442908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = '/mnt/c/spark'\n",
    "print(os.environ.get('SPARK_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T02:43:10.970562Z",
     "start_time": "2024-12-17T02:43:07.719385Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/17 14:58:06 WARN Utils: Your hostname, Laptok resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/12/17 14:58:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/17 14:58:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/17 14:58:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/17 14:58:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName(\"prediction-models\").getOrCreate()\n",
    "df = spark.read.csv(\"preprocessed_datasets/factorized_data.csv\", sep = ',', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cec7f99b724aa1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:45:23.841543Z",
     "start_time": "2024-12-16T22:45:23.309370Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"model_year\", col(\"model_year\").cast(IntegerType())) \\\n",
    "               .withColumn(\"milage\", col(\"milage\").cast(DoubleType())) \\\n",
    "               .withColumn(\"engine_capacity\", col(\"engine_capacity\").cast(DoubleType())) \\\n",
    "               .withColumn(\"engine_horsepower\", col(\"engine_horsepower\").cast(DoubleType())) \\\n",
    "               .withColumn(\"brand_numeric\", col(\"brand_numeric\").cast(IntegerType())) \\\n",
    "               .withColumn(\"transmission_numeric\", col(\"transmission_numeric\").cast(IntegerType())) \\\n",
    "               .withColumn(\"fuel_type_numeric\", col(\"fuel_type_numeric\").cast(IntegerType())) \\\n",
    "               .withColumn(\"ext_col_numeric\", col(\"ext_col_numeric\").cast(IntegerType())) \\\n",
    "               .withColumn(\"int_col_numeric\", col(\"int_col_numeric\").cast(IntegerType())) \\\n",
    "               .withColumn(\"accident_numeric\", col(\"accident_numeric\").cast(IntegerType())) \\\n",
    "                .withColumn(\"price\", col(\"price\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70750e61f855fa1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:45:24.557540Z",
     "start_time": "2024-12-16T22:45:24.553294Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_columns = [\"model_year\", \"milage\", \"engine_capacity\", \"engine_horsepower\", \n",
    "                   \"brand_numeric\", \"transmission_numeric\", \n",
    "                   \"fuel_type_numeric\", \"ext_col_numeric\", \n",
    "                   \"int_col_numeric\", \"accident_numeric\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f0185c97d04675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:45:24.957983Z",
     "start_time": "2024-12-16T22:45:24.929582Z"
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90564e7d6b19cbf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:45:25.508817Z",
     "start_time": "2024-12-16T22:45:25.317592Z"
    }
   },
   "outputs": [],
   "source": [
    "df_vectorized = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d06d2eb6d4923a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:45:25.888841Z",
     "start_time": "2024-12-16T22:45:25.846609Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data = df_vectorized.select(\"features\", \"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9653a183c2fa418c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:45:26.572335Z",
     "start_time": "2024-12-16T22:45:26.552640Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef94b4c345656d1",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "708d0ad6-6499-4d4a-9f7b-007466adab23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:04:18.159958Z",
     "start_time": "2024-12-16T14:01:44.127691Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/17 14:58:23 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/12/17 14:58:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "                                                            "
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.maxIter, [50, 100, 200]) \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.tol, [1e-6, 1e-8]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator to minimize RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "    \n",
    "# Set up CrossValidator for hyperparameter tuning\n",
    "crossval = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter tuning\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "best_model = cv_model.bestModel\n",
    "print(\"Best Parameters:\")\n",
    "print(f\"  MaxIter: {best_model._java_obj.getMaxIter()}\")\n",
    "print(f\"  RegParam: {best_model._java_obj.getRegParam()}\")\n",
    "print(f\"  ElasticNetParam: {best_model._java_obj.getElasticNetParam()}\")\n",
    "print(f\"  Tol: {best_model._java_obj.getTol()}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"features\", \"price\", \"prediction\").show()\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared or Coefficient of Determination (R2): {r2}\")\n",
    "\n",
    "best_model.write().overwrite().save(r\"prediction_models\\LR\")\n",
    "print(\"Best model Linear Regression saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e064b31-9a59-410c-af6b-2ca5de76e809",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9046151261ed6910",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T13:33:00.495562Z",
     "start_time": "2024-12-17T13:32:59.686713Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"price\", seed=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 2, 4]) \\\n",
    "    .addGrid(rf.maxBins, [32, 64, 128]) \\\n",
    "    .addGrid(rf.subsamplingRate, [0.5, 0.7, 1.0]) \\\n",
    "    .addGrid(rf.featureSubsetStrategy, ['all', 'sqrt', 'log2']) \\\n",
    "    .addGrid(rf.maxMemoryInMB, [2048, 4096]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator to minimize RMSE\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=\"price\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "# Set up CrossValidator for hyperparameter tuning\n",
    "crossval = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator_rmse,\n",
    "    numFolds=3  # 3-fold cross-validation\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the hyperparameter tuning\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Best model and parameters\n",
    "best_model = cv_model.bestModel\n",
    "print(\"Best Parameters:\")\n",
    "print(f\"  NumTrees: {best_model.getNumTrees}\")\n",
    "print(f\"  MaxDepth: {best_model.getOrDefault('maxDepth')}\")\n",
    "print(f\"  MinInstancesPerNode: {best_model.getOrDefault('minInstancesPerNode')}\")\n",
    "print(f\"  MaxBins: {best_model.getOrDefault('maxBins')}\")\n",
    "print(f\"  SubsamplingRate: {best_model.getOrDefault('subsamplingRate')}\")\n",
    "print(f\"  FeatureSubsetStrategy: {best_model.getOrDefault('featureSubsetStrategy')}\")\n",
    "print(f\"  MaxMemoryInMB: {best_model.getOrDefault('maxMemoryInMB')}\")\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "predictions = best_model.transform(test_data)\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared or Coefficient of Determination (R2): {r2}\")\n",
    "\n",
    "# Show some predictions\n",
    "predictions.select(\"features\", \"price\", \"prediction\").show()\n",
    "\n",
    "# Save the best model\n",
    "best_model.write().overwrite().save(\"prediction_models/RF_best\")\n",
    "print(\"Best Radnom Forrrest model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1e59e-c20c-4e89-bb3c-9ec35cdc15c8",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11881a59-36ba-4ab2-8067-783cafa18722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:34:04.272798Z",
     "start_time": "2024-12-16T15:06:41.509867Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"price\", seed=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(dt.maxBins, [20, 30, 40, 50]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 2, 4]) \\\n",
    "    .addGrid(dt.minInfoGain, [0.0, 0.1, 0.2]) \\\n",
    "    .addGrid(dt.maxMemoryInMB, [512, 1024, 2048]) \\\n",
    "    .addGrid(dt.cacheNodeIds, [True, False]) \\\n",
    "    .addGrid(dt.checkpointInterval, [10, 20, 30]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator to minimize RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "# Set up CrossValidator for hyperparameter tuning\n",
    "crossval = CrossValidator(\n",
    "    estimator=dt,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5  # 5-fold cross-validation\n",
    ")\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the hyperparameter tuning\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Record the end time and calculate the training time\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Best model and parameters\n",
    "best_model = cv_model.bestModel\n",
    "print(\"Best Parameters:\")\n",
    "print(f\"  MaxDepth: {best_model.getOrDefault('maxDepth')}\")\n",
    "print(f\"  MaxBins: {best_model.getOrDefault('maxBins')}\")\n",
    "print(f\"  MinInstancesPerNode: {best_model.getOrDefault('minInstancesPerNode')}\")\n",
    "print(f\"  MinInfoGain: {best_model.getOrDefault('minInfoGain')}\")\n",
    "print(f\"  MaxMemoryInMB: {best_model.getOrDefault('maxMemoryInMB')}\")\n",
    "print(f\"  CacheNodeIds: {best_model.getOrDefault('cacheNodeIds')}\")\n",
    "print(f\"  CheckpointInterval: {best_model.getOrDefault('checkpointInterval')}\")\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "predictions = best_model.transform(test_data)\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared or Coefficient of Determination (R2): {r2}\")\n",
    "\n",
    "# Show some predictions\n",
    "predictions.select(\"features\", \"price\", \"prediction\").show()\n",
    "\n",
    "# Save the best model\n",
    "best_model.write().overwrite().save(\"prediction_models/DT_best\")\n",
    "print(\"Best Decision Tree model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bac2f4f7debb5e",
   "metadata": {},
   "source": [
    "### How to make a prediction on raw numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f756b7b6cc6f9950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T12:52:37.702832Z",
     "start_time": "2024-12-16T12:52:36.127826Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "# Load a Random Forest model\n",
    "model = RandomForestRegressionModel.load(\"prediction_models/RF_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aadbe851773b484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T12:52:39.390502Z",
     "start_time": "2024-12-16T12:52:38.279555Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_columns = [\"model_year\", \"milage\", \"engine_capacity\", \"engine_horsepower\", \n",
    "                   \"brand_numeric\", \"transmission_numeric\", \n",
    "                   \"fuel_type_numeric\", \"ext_col_numeric\", \n",
    "                   \"int_col_numeric\", \"accident_numeric\"]\n",
    "\n",
    "new_data = spark.createDataFrame([(2007, 213000, 1.6, 150.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)], \n",
    "                                 feature_columns)\n",
    "new_vector = assembler.transform(new_data)\n",
    "\n",
    "prediction = model.transform(new_vector)\n",
    "prediction.select(\"features\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fc3bd381b81400",
   "metadata": {},
   "source": [
    "### How to make a prediction using column mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811c3e1c4eace1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T12:51:17.756076Z",
     "start_time": "2024-12-16T12:51:17.751125Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load mappings from the JSON file\n",
    "with open(\"column_mappings.json\", \"r\") as json_file:\n",
    "    mappings_dict = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8003d92287996ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T12:51:18.307321Z",
     "start_time": "2024-12-16T12:51:18.303038Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_to_index(column_name, value, mappings):\n",
    "    \"\"\"\n",
    "    Maps a categorical value to its corresponding index.\n",
    "    :param column_name: Name of the column (e.g., \"brand\", \"transmission\").\n",
    "    :param value: The raw categorical value to map (e.g., \"Ford\", \"Automatic\").\n",
    "    :param mappings: Dictionary containing the mappings for all categorical columns.\n",
    "    :return: The index of the value in the mapping, or -1 if not found.\n",
    "    \"\"\"\n",
    "    if column_name in mappings:\n",
    "        try:\n",
    "            return mappings[column_name].index(value)\n",
    "        except ValueError:\n",
    "            return -1  # Return -1 if the value is not found\n",
    "    else:\n",
    "        raise KeyError(f\"Column '{column_name}' not found in mappings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83e25782e20b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T12:52:27.087527Z",
     "start_time": "2024-12-16T12:52:27.081999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example raw input with categorical values\n",
    "raw_input = {\n",
    "    \"brand\": \"Ford\",\n",
    "    \"transmission\": \"Automatic\",\n",
    "    \"fuel_type\": \"Gasoline\",\n",
    "    \"ext_col\": \"Black\",\n",
    "    \"int_col\": \"Black\",\n",
    "    \"accident\": \"None reported\"\n",
    "}\n",
    "\n",
    "# Map raw input to indices\n",
    "mapped_input = {\n",
    "    col: map_to_index(col, value, mappings_dict)\n",
    "    for col, value in raw_input.items()\n",
    "}\n",
    "\n",
    "# Add other non-categorical values\n",
    "numeric_input = {\n",
    "    \"model_year\": 2007,\n",
    "    \"milage\": 213000,\n",
    "    \"engine_capacity\": 1.6,\n",
    "    \"engine_horsepower\": 150,\n",
    "}\n",
    "\n",
    "numeric_input.update(mapped_input)\n",
    "\n",
    "print(numeric_input)\n",
    "print(\"Features MUST be in correct order. Model input vectors are created from them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6c805e304c98d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T12:52:29.367335Z",
     "start_time": "2024-12-16T12:52:28.202414Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Define the feature columns (same as in your pipeline)\n",
    "feature_columns = [\"model_year\", \"engine_capacity\", \"engine_horsepower\", \"milage\", \"brand_numeric\", \"transmission_numeric\", \n",
    "                   \"fuel_type_numeric\", \"ext_col_numeric\", \"int_col_numeric\", \"accident_numeric\"]\n",
    "\n",
    "# Convert the mapped input to a Spark DataFrame\n",
    "new_data = spark.createDataFrame([Row(**numeric_input)], schema=feature_columns)\n",
    "\n",
    "# Transform data using the same assembler\n",
    "new_vector = assembler.transform(new_data)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.transform(new_vector)\n",
    "prediction.select(\"features\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33999373b5f1a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
